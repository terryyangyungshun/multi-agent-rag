"""
åŸºæ–¼Agentæ¶æ§‹çš„æº¯æºå¤šæ¨¡æ…‹RAGç³»çµ±
ä½¿ç”¨SequentialAgentçµ„ç¹”RAGæµç¨‹ï¼šè³‡æ–™åŠ è¼‰ â†’ å‘é‡åŒ– â†’ æª¢ç´¢ â†’ å›ç­”ç”Ÿæˆ
"""

import os
import json
from typing import Dict, Any
import numpy as np
import faiss
import mimetypes
from google.adk.agents import Agent, SequentialAgent, ParallelAgent
from google.adk.models.lite_llm import LiteLlm

from dotenv import load_dotenv
load_dotenv(override=True)
QWEN_MODEL = "openai/qwen3-vl:235b-cloud"
QWEN_BASE_URL='http://localhost:11434/v1'
QWEN_API_KEY='your_api_key_here'

# å…¨åŸŸFAISSå„²å­˜
_FAISS_STORAGE = {
    "text_index": None,
    "insight_index": None,
    "text_chunks": None,
    "insights": None,
    "rag_data": None
}

# ================ å·¥å…·å‡½å¼å®šç¾© ================

def load_rag_data_tool(json_data: str) -> Dict[str, Any]:
    """RAGè³‡æ–™åŠ è¼‰å·¥å…· - è™•ç†JSONå­—ä¸²"""
    print(f"[å·¥å…·] è¼‰å…¥RAGè³‡æ–™ï¼Œè³‡æ–™é•·åº¦: {len(json_data)} å­—å…ƒ")
    
    try:
        # è§£æJSONå­—ä¸²
        data = json.loads(json_data)
        
        # çµ±è¨ˆè³‡æ–™é‡
        text_count = len(data.get("text_chunks_with_tracing", []))
        table_count = len(data.get("table_data", []))
        chart_count = len(data.get("chart_data", []))
        insights_count = len(data.get("insights_with_source", []))
        
        load_result = {
            "load_status": "success",
            "data_summary": {
                "text_chunks_count": text_count,
                "table_count": table_count,
                "chart_count": chart_count,
                "insights_count": insights_count
            },
            "document_info": {
                "document_type": data.get("document_classification", {}).get("document_name", "æœªçŸ¥"),
                "summary": data.get("document_summary", "ç„¡æ¦‚è¦"),
                "confidence": data.get("document_classification", {}).get("confidence", 0.0)
            },
            "rag_data": data,  # å®Œæ•´è³‡æ–™å„²å­˜
            "ready_for_vectorization": True,
            "message": f"è³‡æ–™è¼‰å…¥æˆåŠŸ: {text_count}å€‹æ–‡æœ¬å¡Š, {table_count}å€‹è¡¨æ ¼, {insights_count}å€‹æ´å¯Ÿ"
        }
        
        print(f"[å·¥å…·] è³‡æ–™è¼‰å…¥æˆåŠŸ: {text_count}å€‹æ–‡æœ¬å¡Š")
        return load_result
        
    except Exception as e:
        print(f"[å·¥å…·] è³‡æ–™è¼‰å…¥å¤±æ•—: {str(e)}")
        return {
            "load_status": "failed",
            "error": str(e),
            "ready_for_vectorization": False
        }

def vectorize_content_tool(rag_data: Dict[str, Any]) -> Dict[str, Any]:
    """å…§å®¹å‘é‡åŒ–å·¥å…·"""
    global _FAISS_STORAGE
    print(f"[å·¥å…·] é–‹å§‹FAISSå‘é‡åŒ–è™•ç†...")
    
    try:
        vectorization_result = {
            "vectorization_status": "success",
            "vectorized_content": {},
            "indexing_complete": True,
            "ready_for_search": True
        }
        
        # å„²å­˜RAGè³‡æ–™åˆ°å…¨åŸŸè®Šæ•¸
        _FAISS_STORAGE["rag_data"] = rag_data
        
        # 1. å‘é‡åŒ–text_chunks_with_tracingä¸¦å»ºç«‹FAISSç´¢å¼•
        if "text_chunks_with_tracing" in rag_data:
            chunks = rag_data["text_chunks_with_tracing"]
            if chunks:
                print(f"[FAISS] è™•ç† {len(chunks)} å€‹æ–‡æœ¬å¡Š...")
                text_contents = [chunk["content"] for chunk in chunks]
                text_embeddings = get_bailian_embedding(text_contents)
                
                # ç¢ºä¿è³‡æ–™å‹æ…‹ç‚ºfloat32ï¼ˆFAISSè¦æ±‚ï¼‰
                text_embeddings = text_embeddings.astype(np.float32)
                
                # é©—è­‰å‘é‡è³‡æ–™
                if np.any(np.isnan(text_embeddings)) or np.any(np.isinf(text_embeddings)):
                    raise ValueError("å‘é‡åŒ…å«NaNæˆ–ç„¡çª®å¤§å€¼")
                
                # å»ºç«‹FAISSç´¢å¼• (ä½¿ç”¨å…§ç©è¨ˆç®—ï¼Œé©åˆæ­¸ä¸€åŒ–å‘é‡)
                dimension = text_embeddings.shape[1]
                text_index = faiss.IndexFlatIP(dimension)
                
                # å‘é‡æ­¸ä¸€åŒ–ï¼ˆç”¨æ–¼é¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
                faiss.normalize_L2(text_embeddings)
                
                # åŠ å…¥å‘é‡åˆ°ç´¢å¼•
                text_index.add(text_embeddings)
                
                # å„²å­˜åˆ°å…¨åŸŸå„²å­˜ï¼ˆé¿å…åºåˆ—åŒ–å•é¡Œï¼‰
                _FAISS_STORAGE["text_index"] = text_index
                _FAISS_STORAGE["text_chunks"] = chunks
                vectorization_result["vectorized_content"]["text_chunks_vectorized"] = len(chunks)
                
                print(f"[FAISS] æ–‡æœ¬ç´¢å¼•å»ºç«‹å®Œæˆ: {len(chunks)}å€‹å‘é‡, ç¶­åº¦: {dimension}")
        
        # 2. å‘é‡åŒ–insights_with_sourceä¸¦å»ºç«‹FAISSç´¢å¼•
        if "insights_with_source" in rag_data:
            insights = rag_data["insights_with_source"]
            if insights:
                print(f"[FAISS] è™•ç† {len(insights)} å€‹æ´å¯Ÿ...")
                insight_texts = [insight["insight"] for insight in insights]
                insight_embeddings = get_bailian_embedding(insight_texts)
                
                # ç¢ºä¿è³‡æ–™å‹æ…‹ç‚ºfloat32ï¼ˆFAISSè¦æ±‚ï¼‰
                insight_embeddings = insight_embeddings.astype(np.float32)
                
                # é©—è­‰å‘é‡è³‡æ–™
                if np.any(np.isnan(insight_embeddings)) or np.any(np.isinf(insight_embeddings)):
                    raise ValueError("æ´å¯Ÿå‘é‡åŒ…å«NaNæˆ–ç„¡çª®å¤§å€¼")
                
                # å»ºç«‹FAISSç´¢å¼•
                dimension = insight_embeddings.shape[1]
                insight_index = faiss.IndexFlatIP(dimension)
                
                # å‘é‡æ­¸ä¸€åŒ–
                faiss.normalize_L2(insight_embeddings)
                
                # åŠ å…¥å‘é‡åˆ°ç´¢å¼•
                insight_index.add(insight_embeddings)
                
                # å„²å­˜åˆ°å…¨åŸŸå„²å­˜ï¼ˆé¿å…åºåˆ—åŒ–å•é¡Œï¼‰
                _FAISS_STORAGE["insight_index"] = insight_index
                _FAISS_STORAGE["insights"] = insights
                vectorization_result["vectorized_content"]["insights_vectorized"] = len(insights)
                
                print(f"[FAISS] æ´å¯Ÿç´¢å¼•å»ºç«‹å®Œæˆ: {len(insights)}å€‹å‘é‡, ç¶­åº¦: {dimension}")
        
        vectorization_result["vectorized_content"]["embedding_model"] = "bge-m3"
        vectorization_result["vectorized_content"]["vector_dimension"] = dimension if 'dimension' in locals() else 1536
        vectorization_result["vectorized_content"]["index_type"] = "FAISS_IndexFlatIP"
        vectorization_result["vectorized_content"]["storage_method"] = "global_storage"
        vectorization_result["message"] = "FAISSå‘é‡åŒ–å®Œæˆï¼Œç´¢å¼•å·²å„²å­˜åˆ°å…¨åŸŸç©ºé–“"
        
        indices_count = sum(1 for k in ["text_index", "insight_index"] if _FAISS_STORAGE[k] is not None)
        print(f"[FAISS] å‘é‡åŒ–å®Œæˆï¼Œå»ºç«‹äº† {indices_count} å€‹ç´¢å¼•")
        return vectorization_result
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"[å·¥å…·] FAISSå‘é‡åŒ–å¤±æ•—: {str(e)}")
        print(f"[é™¤éŒ¯] è©³ç´°éŒ¯èª¤è³‡è¨Š:\n{error_details}")
        
        return {
            "vectorization_status": "failed",
            "error": str(e),
            "error_details": error_details,
            "ready_for_search": False
        }

def search_with_tracing_tool(query: str) -> Dict[str, Any]:
    """å¸¶æº¯æºçš„FAISSæª¢ç´¢å·¥å…·ï¼ˆä½¿ç”¨å…¨åŸŸå„²å­˜ï¼‰"""
    global _FAISS_STORAGE
    print(f"ğŸ” [FAISS] åŸ·è¡Œæº¯æºæª¢ç´¢: {query}")
    print(f"æ“·å–åˆ°çš„ä½¿ç”¨è€…å•é¡Œï¼š{query}")
    try:
        results = {
            "text_matches": [],
            "table_matches": [], 
            "insight_matches": [],
            "query_analysis": {
                "user_intent": f"ä½¿ç”¨è€…æŸ¥è©¢é—œæ–¼: {query}",
                "search_strategy": "FAISSå‘é‡æª¢ç´¢ + é—œéµè©æ¯”å°",
                "confidence": 0.9
            },
            "search_engine": "FAISS_GLOBAL"
        }
        
        # 1. FAISSæ–‡æœ¬å¡Šæª¢ç´¢
        if (_FAISS_STORAGE["text_index"] is not None and 
            _FAISS_STORAGE["text_chunks"] is not None):
            
            text_index = _FAISS_STORAGE["text_index"]
            chunks = _FAISS_STORAGE["text_chunks"]
            
            print(f"ğŸ”¤ [FAISS] æª¢ç´¢æ–‡æœ¬å¡Šï¼Œç´¢å¼•å¤§å°: {text_index.ntotal}")
            
            # æŸ¥è©¢å‘é‡åŒ–ä¸¦æ­¸ä¸€åŒ–
            query_embedding = get_bailian_embedding([query])
            query_embedding = query_embedding.astype(np.float32)  # FAISSè¦æ±‚float32
            faiss.normalize_L2(query_embedding)
            
            # FAISSæª¢ç´¢
            top_k = min(5, text_index.ntotal)  # æª¢ç´¢top-kå€‹çµæœ
            similarities, indices = text_index.search(query_embedding, top_k)
            
            # è™•ç†æª¢ç´¢çµæœ
            for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                if idx != -1 and similarity > 0.3:  # FAISSè¿”å›çš„æ˜¯é¤˜å¼¦ç›¸ä¼¼åº¦
                    chunk = chunks[idx]
                    results["text_matches"].append({
                        "chunk_id": chunk["chunk_id"],
                        "content": chunk["content"],
                        "similarity": float(similarity),
                        "source_type": chunk["source_type"],
                        "position": chunk["position"],
                        "confidence": chunk["confidence"],
                        "faiss_rank": i + 1
                    })
            
            print(f"âœ… [FAISS] æ–‡æœ¬æª¢ç´¢å®Œæˆ: {len(results['text_matches'])}å€‹åŒ¹é…")
        
        # 2. å‚³çµ±é—œéµè©æª¢ç´¢è¡¨æ ¼è³‡æ–™
        if (_FAISS_STORAGE["rag_data"] is not None and 
            "table_data" in _FAISS_STORAGE["rag_data"]):
            
            rag_data = _FAISS_STORAGE["rag_data"]
            for table in rag_data["table_data"]:
                if any(keyword in table["description"].lower() or keyword in table["extracted_content"].lower() 
                       for keyword in query.lower().split()):
                    results["table_matches"].append({
                        "table_id": table["table_id"],
                        "description": table["description"],
                        "relevance": 0.8,
                        "match_type": "keyword"
                    })
            
            print(f"ğŸ“Š [é—œéµè©] è¡¨æ ¼æª¢ç´¢å®Œæˆ: {len(results['table_matches'])}å€‹åŒ¹é…")
        
        # 3. FAISSæ´å¯Ÿæª¢ç´¢
        if (_FAISS_STORAGE["insight_index"] is not None and 
            _FAISS_STORAGE["insights"] is not None):
            
            insight_index = _FAISS_STORAGE["insight_index"]
            insights = _FAISS_STORAGE["insights"]
            
            print(f"ğŸ’¡ [FAISS] æª¢ç´¢æ´å¯Ÿï¼Œç´¢å¼•å¤§å°: {insight_index.ntotal}")
            
            # æŸ¥è©¢å‘é‡åŒ–ä¸¦æ­¸ä¸€åŒ–
            query_embedding = get_bailian_embedding([query])
            query_embedding = query_embedding.astype(np.float32)  # FAISSè¦æ±‚float32
            faiss.normalize_L2(query_embedding)
            
            # FAISSæª¢ç´¢
            top_k = min(3, insight_index.ntotal)
            similarities, indices = insight_index.search(query_embedding, top_k)
            
            # è™•ç†æª¢ç´¢çµæœ
            for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                if idx != -1 and similarity > 0.3:
                    insight = insights[idx]
                    results["insight_matches"].append({
                        "insight": insight["insight"],
                        "source_evidence": insight["source_evidence"],
                        "confidence": insight["confidence"],
                        "similarity": float(similarity),
                        "insight_type": insight.get("insight_type", "general"),
                        "faiss_rank": i + 1
                    })
            
            print(f"âœ… [FAISS] æ´å¯Ÿæª¢ç´¢å®Œæˆ: {len(results['insight_matches'])}å€‹åŒ¹é…")
        
        # çµ±è¨ˆçµæœ
        results["total_matches"] = len(results["text_matches"]) + len(results["table_matches"]) + len(results["insight_matches"])
        results["search_quality"] = "high" if results["total_matches"] >= 3 else "medium" if results["total_matches"] >= 1 else "low"
        results["ready_for_answer"] = results["total_matches"] > 0
        
        print(f"ğŸ¯ [FAISS] æª¢ç´¢å®Œæˆ: {results['total_matches']}å€‹æ¯”å°çµæœ")
        print(f"    ğŸ“ æ–‡æœ¬: {len(results['text_matches'])} | ğŸ“Š è¡¨æ ¼: {len(results['table_matches'])} | ğŸ’¡ æ´å¯Ÿ: {len(results['insight_matches'])}")
        
        return results
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"âŒ [FAISS] æª¢ç´¢å¤±æ•—: {str(e)}")
        print(f"ğŸ” [é™¤éŒ¯] è©³ç´°éŒ¯èª¤è³‡è¨Š:\n{error_details}")
        
        return {
            "error": str(e),
            "error_details": error_details,
            "total_matches": 0,
            "ready_for_answer": False,
            "search_engine": "FAISS_ERROR"
        }

# ================ RAGæ™ºèƒ½é«”å®šç¾© ================

# 1. è³‡æ–™åŠ è¼‰å°ˆå®¶
data_loader_agent = Agent(
    name="data_loader_agent",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="RAGè³‡æ–™åŠ è¼‰èˆ‡é è™•ç†å°ˆå®¶",
    instruction="""
            ä½ æ˜¯RAGè³‡æ–™åŠ è¼‰å°ˆå®¶ã€‚å¾ä½¿ç”¨è€…è¨Šæ¯ä¸­æ“·å–JSONè³‡æ–™å­—ä¸²ï¼Œä½¿ç”¨load_rag_data_toolå·¥å…·è§£æè³‡æ–™ã€‚

            ä»»å‹™æ­¥é©Ÿï¼š
            1. å¾ä½¿ç”¨è€…è¨Šæ¯ä¸­è­˜åˆ¥ä¸¦æ“·å–JSONè³‡æ–™å­—ä¸²
            2. å‘¼å«load_rag_data_tool(json_data_string)è§£æJSONè³‡æ–™
            3. ç¸½çµåŠ è¼‰çµæœ

            è¼¸å‡ºåŠ è¼‰ç‹€æ…‹å ±å‘Šï¼š
            - è³‡æ–™åŠ è¼‰ç‹€æ…‹ï¼ˆæˆåŠŸ/å¤±æ•—ï¼‰
            - æ–‡ä»¶é¡å‹èˆ‡æ¦‚è¦  
            - å„é¡è³‡æ–™çµ±è¨ˆï¼ˆæ–‡æœ¬å¡Šã€è¡¨æ ¼ã€æ´å¯Ÿæ•¸é‡ï¼‰
            - æ˜¯å¦æº–å‚™å¥½é€²è¡Œå‘é‡åŒ–è™•ç†

            æ³¨æ„ï¼šå‚³å…¥å·¥å…·çš„å¿…é ˆæ˜¯å®Œæ•´çš„JSONå­—ä¸²ï¼Œä¸æ˜¯æª”æ¡ˆè·¯å¾‘ã€‚
            è³‡æ–™åŠ è¼‰å®Œæˆå¾Œå°‡å„²å­˜åœ¨session.stateä¸­ä¾›å¾ŒçºŒä½¿ç”¨ã€‚
            """,
    tools=[load_rag_data_tool],
    output_key="data_loading_result"
)

# 2. å‘é‡åŒ–å°ˆå®¶
vectorization_agent = Agent(
    name="vectorization_agent", 
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="FAISSå‘é‡åŒ–å°ˆå®¶",
    instruction="""
        ä½ æ˜¯FAISSå‘é‡åŒ–å°ˆå®¶ã€‚å¾session.state['data_loading_result']å–å¾—å·²åŠ è¼‰çš„RAGè³‡æ–™ï¼Œä½¿ç”¨vectorize_content_toolå·¥å…·å»ºç«‹FAISSç´¢å¼•ã€‚

        ä»»å‹™æ­¥é©Ÿï¼š
        1. å¾session.stateä¸­å–å¾—data_loading_result
        2. æ“·å–å…¶ä¸­çš„rag_data
        3. å‘¼å«vectorize_content_tool(rag_data)é€²è¡ŒFAISSå‘é‡åŒ–
        4. ç¸½çµFAISSç´¢å¼•å»ºç«‹çµæœ

        FAISSå‘é‡åŒ–ç‰¹é»ï¼š
        - ä½¿ç”¨Ollama Embedding APIç”Ÿæˆå‘é‡ï¼ˆæ”¯æ´åˆ†æ‰¹è™•ç†ï¼Œæ¯æ‰¹æœ€å¤š25å€‹ï¼‰
        - å»ºç«‹IndexFlatIPç´¢å¼•ï¼ˆé©åˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
        - å‘é‡L2æ­¸ä¸€åŒ–è™•ç†
        - **é‡è¦**: ç´¢å¼•å„²å­˜åœ¨å…¨åŸŸç©ºé–“ï¼Œé¿å…ADKåºåˆ—åŒ–å•é¡Œ

        å‘¼å«å·¥å…·å¾Œï¼Œè¼¸å‡ºå‘é‡åŒ–ç‹€æ…‹å ±å‘Šï¼š
        - FAISSç´¢å¼•å»ºç«‹ç‹€æ…‹
        - è™•ç†çš„å…§å®¹çµ±è¨ˆï¼ˆæ–‡æœ¬å¡Šæ•¸é‡ã€æ´å¯Ÿæ•¸é‡ï¼‰
        - å‘é‡ç¶­åº¦å’Œç´¢å¼•é¡å‹è³‡è¨Š
        - å…¨åŸŸå„²å­˜ç‹€æ…‹
        - æ˜¯å¦æº–å‚™å¥½é€²è¡ŒFAISSæª¢ç´¢

        FAISSç´¢å¼•å·²å®‰å…¨å„²å­˜åˆ°å…¨åŸŸç©ºé–“ï¼Œä¾›å¾ŒçºŒæª¢ç´¢ä½¿ç”¨ã€‚
""",
    tools=[vectorize_content_tool],
    output_key="vectorization_result"
)

# 3. æª¢ç´¢å°ˆå®¶
retrieval_agent = Agent(
    name="retrieval_agent",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="FAISSå¤šæ¨¡æ…‹æª¢ç´¢å°ˆå®¶ - æ”¯æ´ç²¾ç¢ºæº¯æº",
    instruction="""
ä½ æ˜¯FAISSå¤šæ¨¡æ…‹æª¢ç´¢å°ˆå®¶ï¼Œä½¿ç”¨search_with_tracing_toolåŸ·è¡Œé«˜æ•ˆçš„å‘é‡æª¢ç´¢ã€‚

**é‡è¦**: FAISSç´¢å¼•å·²å„²å­˜åœ¨å…¨åŸŸç©ºé–“ï¼Œå·¥å…·å‡½å¼æœƒè‡ªå‹•è¨ªå•ã€‚

ä»»å‹™æ­¥é©Ÿï¼š
1. å¾ä½¿ç”¨è€…è¨Šæ¯ä¸­æ“·å–æŸ¥è©¢å•é¡Œ
2. å‘¼å«search_with_tracing_tool(query)åŸ·è¡ŒFAISSå‘é‡æª¢ç´¢
3. åˆ†ææª¢ç´¢çµæœå“è³ª

FAISSæª¢ç´¢ç‰¹é»ï¼š
- æ–‡æœ¬å¡Šï¼šä½¿ç”¨FAISS IndexFlatIPé€²è¡Œé¤˜å¼¦ç›¸ä¼¼åº¦æª¢ç´¢
- è¡¨æ ¼è³‡æ–™ï¼šä½¿ç”¨å‚³çµ±é—œéµè©æ¯”å°
- æ´å¯Ÿè³‡è¨Šï¼šä½¿ç”¨FAISS IndexFlatIPé€²è¡Œèªç¾©æª¢ç´¢
- æ‰€æœ‰çµæœéƒ½åŒ…å«faiss_rankæ’åºè³‡è¨Š
- ç´¢å¼•å„²å­˜åœ¨å…¨åŸŸç©ºé–“ï¼Œé¿å…åºåˆ—åŒ–å•é¡Œ

å‘¼å«å·¥å…·å¾Œï¼Œè¼¸å‡ºæª¢ç´¢ç‹€æ…‹å ±å‘Šï¼š
- æŸ¥è©¢æ„åœ–åˆ†æ
- FAISSæª¢ç´¢çµæœçµ±è¨ˆï¼ˆæ–‡æœ¬/è¡¨æ ¼/æ´å¯ŸåŒ¹é…æ•¸é‡ï¼‰
- æª¢ç´¢å“è³ªè©•ä¼°å’Œæ’åºè³‡è¨Š
- æ˜¯å¦æº–å‚™å¥½ç”Ÿæˆå›ç­”

ç¢ºä¿æª¢ç´¢çµæœå®Œæ•´ä¸¦åŒ…å«æº¯æºè³‡è¨Šå’ŒFAISSæ’åºã€‚
""",
    tools=[search_with_tracing_tool],
    output_key="retrieval_result"
)

# 4. å›ç­”ç”Ÿæˆå°ˆå®¶
answer_generation_agent = Agent(
    name="answer_generation_agent",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="FAISSæº¯æºå›ç­”ç”Ÿæˆå°ˆå®¶",
    instruction="""
ä½ æ˜¯FAISSæº¯æºå›ç­”ç”Ÿæˆå°ˆå®¶ï¼ŒåŸºæ–¼session.stateä¸­çš„FAISSæª¢ç´¢çµæœç”Ÿæˆå¸¶æœ‰ç²¾ç¢ºä¾†æºè³‡è¨Šçš„å›ç­”ã€‚

å¾session.stateå–å¾—ï¼š
- 'data_loading_result': æ–‡æª”è³‡è¨Š
- 'vectorization_result': FAISSå‘é‡åŒ–ç‹€æ…‹ï¼ˆå…¨åŸŸå„²å­˜ï¼‰
- 'retrieval_result': FAISSæª¢ç´¢çµæœï¼ˆåŒ…å«text_matches, table_matches, insight_matchesï¼‰

FAISSæª¢ç´¢çµæœç‰¹é»ï¼š
- text_matches: åŒ…å«faiss_rankæ’åºï¼Œsimilarityç‚ºé¤˜å¼¦ç›¸ä¼¼åº¦
- table_matches: é—œéµè©åŒ¹é…ï¼ŒåŒ…å«match_type
- insight_matches: åŒ…å«faiss_rankæ’åºå’Œinsight_type
- search_engine: "FAISS_GLOBAL" è¡¨ç¤ºä½¿ç”¨å…¨åŸŸå„²å­˜

ä»»å‹™ï¼š
1. åˆ†æFAISSæª¢ç´¢åˆ°çš„å„é¡åŒ¹é…çµæœ
2. åŸºæ–¼åŒ¹é…å…§å®¹å’ŒFAISSæ’åºç”Ÿæˆæº–ç¢ºå›ç­”
3. æä¾›å®Œæ•´çš„æº¯æºè³‡è¨Šï¼ˆchunk_idã€ä½ç½®ã€FAISSæ’åºã€ç›¸ä¼¼åº¦ç­‰ï¼‰
4. è©•ä¼°å›ç­”çš„å¯ä¿¡åº¦

è¼¸å‡ºæ ¼å¼ï¼š
**ç­”æ¡ˆæ¦‚è¦**: [ä¸€å¥è©±å›ç­”ä½¿ç”¨è€…å•é¡Œ]

**è©³ç´°è§£ç­”**: 
[åŸºæ–¼FAISSæª¢ç´¢åˆ°çš„ä¿¡æ¯é€²è¡Œè©³ç´°å›ç­”ï¼Œå¼•ç”¨å…·é«”å…§å®¹å’Œæ’åº]

**è³‡è¨Šä¾†æº**:
- ä¾†æº1: [chunk_id] - [source_type] - [position] - [å…§å®¹æ¦‚è¦] (FAISSç›¸ä¼¼åº¦: 0.95, æ’åº: #1)
- ä¾†æº2: [table_id] - [è¡¨æ ¼æè¿°] (é—œéµè©åŒ¹é…)
- ä¾†æº3: [æ´å¯Ÿå…§å®¹] (FAISSç›¸ä¼¼åº¦: 0.85, æ’åº: #2, é¡å‹: trend)

**ç›¸é—œè³‡æ–™**:
[å¦‚æœæœ‰çµæ§‹åŒ–è³‡æ–™ï¼Œåˆ—å‡ºé—œéµè³‡è¨Š]

**è£œå……å»ºè­°**:
[åŸºæ–¼å…§å®¹æä¾›å¯¦ç”¨å»ºè­°]

**å›ç­”å¯ä¿¡åº¦**: [high/medium/low] (åŸºæ–¼FAISSæ’åºã€ç›¸ä¼¼åº¦å’Œä¾†æºæ•¸é‡ç¶œåˆè©•ä¼°)

æ³¨æ„ï¼šç¢ºä¿æ¯å€‹è³‡è¨Šéƒ½èƒ½ç²¾ç¢ºæº¯æºåˆ°åŸå§‹ä½ç½®ï¼ŒåŒ…å«FAISSæ’åºè³‡è¨Šå’Œç›¸ä¼¼åº¦åˆ†æ•¸ã€‚
""",
    output_key="final_answer"
)


# ================ åŸºç¤å·¥å…·å‡½å¼ ================

def get_bailian_embedding(texts: list[str]) -> np.ndarray:
    """èª¿ç”¨ LangChain Ollama Embeddings APIï¼ˆè¢«å·¥å…·å‡½å¼å‘¼å«ï¼‰- æ”¯æ´åˆ†æ‰¹è™•ç†"""
    try:
        if not texts:
            raise ValueError("æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ç‚ºç©º")
        
        # éæ¿¾ç©ºæ–‡æœ¬
        valid_texts = [text.strip() for text in texts if text and text.strip()]
        if not valid_texts:
            raise ValueError("æ²’æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å…§å®¹")
        
        print(f"[å‘é‡åŒ–] è™•ç† {len(valid_texts)} å€‹æ–‡æœ¬...")

        # åŒ¯å…¥ langchain_community.embeddings
        from langchain_community.embeddings import OllamaEmbeddings
        import numpy as np

        # æ¯æ‰¹æœ€å¤š25å€‹æ–‡æœ¬
        BATCH_SIZE = 25
        all_embeddings = []

        # å»ºç«‹ Ollama Embeddings ç‰©ä»¶
        embeddings = OllamaEmbeddings(model="bge-m3")

        # åˆ†æ‰¹è™•ç†
        for i in range(0, len(valid_texts), BATCH_SIZE):
            batch_texts = valid_texts[i:i + BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            total_batches = (len(valid_texts) + BATCH_SIZE - 1) // BATCH_SIZE

            print(f"[æ‰¹æ¬¡ {batch_num}/{total_batches}] è™•ç† {len(batch_texts)} å€‹æ–‡æœ¬...")

            # å‘¼å« Ollama Embeddings å–å¾—å‘é‡
            batch_embeddings = embeddings.embed_documents(batch_texts)
            all_embeddings.extend(batch_embeddings)
            print(f"[æ‰¹æ¬¡ {batch_num}] æˆåŠŸç²å– {len(batch_embeddings)} å€‹å‘é‡")

        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡çš„å‘é‡
        embeddings_array = np.array(all_embeddings, dtype=np.float64)

        # é©—è­‰å‘é‡
        if embeddings_array.size == 0:
            raise ValueError("ç²å–åˆ°ç©ºçš„å‘é‡")

        print(f"[Ollamaå‘é‡åŒ–] æˆåŠŸç²å–å‘é‡ï¼Œshape: {embeddings_array.shape}")
        print(f"[çµ±è¨ˆ] ç¸½è¨ˆ {len(all_embeddings)} å€‹å‘é‡ï¼Œç¶­åº¦: {embeddings_array.shape[1]}")

        return embeddings_array

    except Exception as e:
        print(f"[å‘é‡åŒ–] å¤±æ•—: {str(e)}")
        raise e

# ================ å·¥å…·å‡½å¼ ================
def load_local_image(image_path: str) -> tuple[bytes, str]:
    """åŠ è¼‰æœ¬åœ°åœ–ç‰‡"""
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"åœ–ç‰‡æª”æ¡ˆä¸å­˜åœ¨: {image_path}")
    
    with open(image_path, 'rb') as f:
        image_data = f.read()
    
    mime_type, _ = mimetypes.guess_type(image_path)
    if not mime_type or not mime_type.startswith('image/'):
        mime_type = 'image/jpeg'
    
    print(f"åŠ è¼‰åœ–ç‰‡: {image_path}")
    print(f"æª”æ¡ˆå¤§å°: {len(image_data)} bytes")
    
    return image_data, mime_type

# ================ æ™ºèƒ½é«”å®šç¾© ================

# æ–‡æª”é¡å‹å®šç¾©
SUPPORTED_DOCUMENT_TYPES = {
    "financial_report": "è²¡å‹™å ±è¡¨",
    "invoice": "ç™¼ç¥¨ç¥¨æ“š", 
    "contract": "åˆåŒæ–‡æª”",
    "research_paper": "ç ”ç©¶å ±å‘Š",
    "business_chart": "å•†æ¥­åœ–è¡¨",
    "receipt": "æ”¶æ“šæ†‘è­‰",
    "form": "è¡¨å–®æ–‡æª”",
    "presentation": "æ¼”ç¤ºæ–‡æª”",
    "course_material": "èª²ç¨‹è³‡æ–™",
    "technical_doc": "æŠ€è¡“æ–‡æª”",
    "other": "å…¶ä»–æ–‡æª”"
}

# 0. æ–‡æª”åˆ†é¡å°ˆå®¶
document_classifier = Agent(
    name="document_classifier",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY,
    ),
    description="æ–‡æª”é¡å‹è­˜åˆ¥å’Œåˆ†é¡å°ˆå®¶",
    instruction=f"""
            ä½ æ˜¯å°ˆæ¥­çš„æ–‡æª”åˆ†é¡å°ˆå®¶ã€‚è«‹åˆ†æåœ–åƒä¸¦è­˜åˆ¥æ–‡æª”é¡å‹ã€‚

            æ”¯æŒçš„æ–‡æª”é¡å‹ï¼š
            {json.dumps(SUPPORTED_DOCUMENT_TYPES, ensure_ascii=False, indent=2)}

            ä»»å‹™ï¼š
            1. è­˜åˆ¥æ–‡æª”çš„ä¸»è¦é¡å‹
            2. ä¼°ç®—è­˜åˆ¥çš„ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
            3. è­˜åˆ¥æ–‡æª”çš„èªè¨€å’Œæ ¼å¼
            4. æª¢æ¸¬æ–‡æª”çš„å“è³ªå’Œæ¸…æ™°åº¦

            è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼JSONï¼‰ï¼š
            {{
                "document_type": "é¡å‹ä»£ç¢¼ï¼ˆå¦‚ï¼šcourse_materialï¼‰",
                "document_name": "é¡å‹ä¸­æ–‡åç¨±ï¼ˆå¦‚ï¼šèª²ç¨‹è³‡æ–™ï¼‰",
                "confidence": 0.95,
                "language": "ä¸­æ–‡/è‹±æ–‡/æ··åˆ",
                "format": "è¡¨æ ¼/æ–‡æœ¬/åœ–è¡¨/æ··åˆ",
                "quality": "high/medium/low",
                "reasoning": "åˆ†é¡ç†ç”±å’Œä¾æ“š"
            }}
            """,
    output_key="document_classification",
)

# 1. æ–‡æœ¬æå–å°ˆå®¶
text_extractor = Agent(
    name="text_extractor",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="OCRæ–‡æœ¬æå–å°ˆå®¶",
    instruction="""
            ä½ æ˜¯å°ˆæ¥­çš„OCRæ–‡æœ¬æå–å°ˆå®¶ã€‚è«‹ä»”ç´°æå–åœ–ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ä¸¦é€²è¡Œçµæ§‹åŒ–æ¨™è¨»ã€‚

            ä»»å‹™ï¼š
            1. æå–æ‰€æœ‰å¯è¦‹æ–‡å­—ï¼ŒåŒ…æ‹¬è¡¨æ ¼ã€æ¨™é¡Œã€æ­£æ–‡
            2. è­˜åˆ¥æ–‡å­—çš„é¡å‹å’Œé‡è¦æ€§
            3. ä¼°ç®—æ¯å€‹æ–‡æœ¬å¡Šçš„ç½®ä¿¡åº¦
            4. æ¨™è¨»æ–‡å­—çš„å¤§æ¦‚ä½ç½®

            è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼JSONï¼‰ï¼š
            {{
                "extracted_texts": [
                    {{
                        "text": "å…·é«”æ–‡å­—å…§å®¹",
                        "position": "å·¦ä¸Šè§’/ä¸­å¤®/å³ä¸‹è§’/é ‚éƒ¨/åº•éƒ¨ç­‰",
                        "text_type": "title/subtitle/data/label/content/table/header",
                        "confidence": 0.95,
                        "importance": "high/medium/low"
                    }}
                ],
                "total_confidence": 0.88,
                "extraction_summary": "æå–æ¦‚è¿°",
                "text_count": 15,
                "quality_assessment": "æå–å“è³ªè©•ä¼°"
            }}
            """,
    output_key="text_extraction",
)

# 2. ä½ˆå±€åˆ†æå°ˆå®¶
layout_analyzer = Agent(
    name="layout_analyzer", 
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="æ–‡æª”çµæ§‹å’Œä½ˆå±€åˆ†æå°ˆå®¶",
    instruction="""
            ä½ æ˜¯å°ˆæ¥­çš„æ–‡æª”çµæ§‹åˆ†æå°ˆå®¶ã€‚è«‹åˆ†ææ–‡æª”çš„ä½ˆå±€å’Œçµæ§‹ã€‚

            ä»»å‹™ï¼š
            1. è­˜åˆ¥æ–‡æª”çš„æ•´é«”ä½ˆå±€çµæ§‹
            2. æª¢æ¸¬è¡¨æ ¼ã€åœ–è¡¨ã€æ–‡æœ¬å¡Šçš„ä½ç½®
            3. åˆ†æè³‡è¨Šçš„å±¤æ¬¡é—œä¿‚
            4. è­˜åˆ¥é—œéµå€åŸŸå’Œé‡é»å…§å®¹

            è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼JSONï¼‰ï¼š
            {{
                "layout_type": "è¡¨æ ¼å‹/å ±å‘Šå‹/åœ–è¡¨å‹/æ··åˆå‹/èª²ç¨‹å‹",
                "structure_elements": [
                    {{
                        "element_type": "header/table/chart/text_block/footer/title/section",
                        "position": "ä½ç½®æè¿°ï¼ˆå¦‚ï¼šé ‚éƒ¨/å·¦ä¸Šè§’/ä¸­å¤®ç­‰ï¼‰",
                        "size": "large/medium/small",
                        "importance": "high/medium/low",
                        "description": "å…ƒç´ æè¿°"
                    }}
                ],
                "visual_hierarchy": "æè¿°è¦–è¦ºå±¤æ¬¡å’Œè³‡è¨Šçµ„ç¹”æ–¹å¼",
                "key_regions": ["é‡é»å€åŸŸ1", "é‡é»å€åŸŸ2"],
                "layout_complexity": "simple/moderate/complex",
                "analysis_confidence": 0.87
            }}
            """,
    output_key="layout_analysis",
)

# åœ–åƒå…§å®¹ä¸¦è¡Œæå–ï¼ˆOCR + ä½ˆå±€åˆ†æï¼‰
image_extractor_agent = ParallelAgent(
    name="image_extractor_workflow", 
    sub_agents=[text_extractor, layout_analyzer],
    description="ä¸¦è¡Œé€²è¡ŒOCRæ–‡æœ¬æå–å’Œä½ˆå±€çµæ§‹åˆ†æ",
)

# 3. å…§å®¹ç†è§£å°ˆå®¶  
content_analyzer = Agent(
    name="content_analyzer",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="å…§å®¹ç†è§£å’Œè³‡æ–™æå–å°ˆå®¶",
    instruction="""
            ä½ æ˜¯å°ˆæ¥­çš„å…§å®¹ç†è§£å°ˆå®¶ã€‚åŸºæ–¼å‰é¢å°ˆå®¶çš„åˆ†æçµæœï¼Œé€²è¡Œæ·±åº¦å…§å®¹ç†è§£å’Œè³‡æ–™æå–ã€‚

            ä½ å¯ä»¥å¾ {document_classification} ä¸­ç²å–æ–‡æª”åˆ†é¡è³‡è¨Š
            ä½ å¯ä»¥å¾ {text_extraction} ä¸­ç²å–æå–çš„æ–‡å­—è³‡è¨Š
            ä½ å¯ä»¥å¾ {layout_analysis} ä¸­ç²å–ä½ˆå±€åˆ†æè³‡è¨Š

            ä»»å‹™ï¼š
            1. åˆ¤æ–·åœ–åƒçš„ä¸»è¦ç”¨é€”å’Œåƒ¹å€¼
            2. è­˜åˆ¥é—œéµè³‡è¨Šå’Œé‡è¦è³‡æ–™
            3. æå–çµæ§‹åŒ–çš„éµå€¼å°è³‡è¨Š
            4. åˆ†æè³‡æ–™è¶¨å‹¢æˆ–é—œè¯æ€§ï¼ˆå¦‚æœ‰ï¼‰
            5. ç¸½çµæ ¸å¿ƒè¦é»å’Œç™¼ç¾

            è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼JSONï¼‰ï¼š
            {{
                "content_purpose": "æ–‡æª”çš„ä¸»è¦ç”¨é€”å’Œç›®æ¨™",
                "key_information": [
                    "é—œéµè³‡è¨Šé»1",
                    "é—œéµè³‡è¨Šé»2",
                    "é—œéµè³‡è¨Šé»3"
                ],
                "key_value_pairs": {{
                    "é‡è¦å­—æ®µ1": "å€¼1",
                    "é‡è¦å­—æ®µ2": "å€¼2",
                    "æ—¥æœŸ": "2024-01-15",
                    "æ•¸é‡/é‡‘é¡": "å…·é«”æ•¸å€¼"
                }},
                "data_insights": [
                    "è³‡æ–™æ´å¯Ÿ1ï¼šè¶¨å‹¢åˆ†æ",
                    "è³‡æ–™æ´å¯Ÿ2ï¼šé—œè¯ç™¼ç¾"
                ],
                "tables_detected": [
                    {{
                        "table_description": "è¡¨æ ¼æè¿°",
                        "key_data": "è¡¨æ ¼ä¸­çš„é—œéµè³‡æ–™",
                        "importance": "high/medium/low"
                    }}
                ],
                "quality_assessment": {{
                    "information_completeness": "high/medium/low",
                    "data_reliability": "high/medium/low",
                    "analysis_confidence": 0.85
                }},
                "summary": "æ•´é«”å…§å®¹ç¸½çµå’Œæ ¸å¿ƒè¦é»"
            }}
            """,
    output_key="content_analysis",
)

# å®Œæ•´çš„å¤šæ™ºèƒ½é«”åˆ†æå·¥ä½œæµ
# æµç¨‹ï¼šæ–‡æª”åˆ†é¡ â†’ ä¸¦è¡Œæå–(OCR+ä½ˆå±€) â†’ å…§å®¹ç†è§£ â†’ RAGè³‡æ–™æ•´ç†

complete_analysis_workflow = SequentialAgent(
    name="complete_analysis_workflow",
    sub_agents=[
        document_classifier,      # æ­¥é©Ÿ1ï¼šæ–‡æª”åˆ†é¡è­˜åˆ¥
        image_extractor_agent,    # æ­¥é©Ÿ2ï¼šä¸¦è¡Œæå–(OCR + ä½ˆå±€åˆ†æ)
        content_analyzer,         # æ­¥é©Ÿ3ï¼šå…§å®¹ç†è§£å’Œè³‡æ–™æå–
    ],
    description="å®Œæ•´çš„å¤šæ™ºèƒ½é«”æ–‡æª”åˆ†æå·¥ä½œæµï¼šåˆ†é¡ â†’ æå– â†’ ç†è§£",
)

# 4. RAGè³‡æ–™æ•´ç†å°ˆå®¶ï¼ˆæ”¯æ´æº¯æºï¼‰
rag_data_organizer = Agent(
    name="rag_data_organizer",
    model=LiteLlm(
        model=QWEN_MODEL,
        base_url=QWEN_BASE_URL,
        api_key=QWEN_API_KEY
    ),
    description="RAGè³‡æ–™æ•´ç†å°ˆå®¶ - æ”¯æ´ç²¾ç¢ºæº¯æºçš„å¤šæ¨¡æ…‹è³‡æ–™çµæ§‹",
    instruction="""
        ä½ æ˜¯é«˜ç´šRAGè³‡æ–™æ•´ç†å°ˆå®¶ï¼Œå°ˆé–€è² è²¬ç”Ÿæˆæ”¯æ´ç²¾ç¢ºæº¯æºçš„å¤šæ¨¡æ…‹RAGè³‡æ–™çµæ§‹ã€‚

        ä½ å¯ä»¥å¾ session.state ä¸­ç²å–ï¼š
        - 'document_classification': æ–‡æª”åˆ†é¡è³‡è¨Š
        - 'text_extraction': OCRæ–‡æœ¬æå–çµæœ (åŒ…å«extracted_textsæ•¸çµ„)
        - 'layout_analysis': ä½ˆå±€çµæ§‹åˆ†æçµæœ (åŒ…å«structure_elementsæ•¸çµ„)
        - 'content_analysis': å…§å®¹ç†è§£å’Œè³‡æ–™æå–çµæœ

        **é—œéµä»»å‹™**ï¼š
        1. å¾text_extraction.extracted_textsä¸­æå–**çœŸå¯¦æ–‡æœ¬å…§å®¹**ç”Ÿæˆtext_chunks
        2. ç‚ºæ¯å€‹å…§å®¹å¡Šæ·»åŠ **æº¯æºè³‡è¨Š**ï¼ˆä½ç½®ã€é¡å‹ã€ç½®ä¿¡åº¦ï¼‰
        3. **åˆ†é¡è™•ç†**ä¸åŒé¡å‹çš„å…§å®¹ï¼ˆæ–‡æœ¬/è¡¨æ ¼/åœ–è¡¨ï¼‰
        4. æ§‹å»º**å¯è¿½æº¯**çš„è³‡æ–™çµæ§‹

        è¼¸å‡ºæ ¼å¼ï¼ˆåš´æ ¼JSONï¼‰ï¼š
        {{
            "document_summary": "ä¸€å¥è©±æ¦‚æ‹¬åœ–åƒå…§å®¹å’Œåƒ¹å€¼",
            "document_classification": {{
                "document_type": "å¾document_classificationä¸­æå–çš„é¡å‹ä»£ç¢¼",
                "document_name": "æ–‡æª”é¡å‹ä¸­æ–‡åç¨±", 
                "confidence": 0.95,
                "reasoning": "åˆ†é¡ç†ç”±"
            }},
            "extracted_text": "æ‰€æœ‰æå–æ–‡æœ¬çš„å®Œæ•´æ‹¼æ¥å…§å®¹",
            "key_data": {{
                "å¾content_analysis.key_value_pairsä¸­æå–çœŸå¯¦çš„éµå€¼å°": "çœŸå¯¦å€¼",
                "é‡è¦è³‡è¨Š": "å¯¦éš›æå–çš„è³‡è¨Š"
            }},
            "text_chunks_with_tracing": [
                {{
                    "chunk_id": "chunk_001",
                    "content": "å¾extracted_textsä¸­æå–çš„çœŸå¯¦æ–‡æœ¬å…§å®¹",
                    "source_type": "title/content/table/chart/header/section",
                    "position": "å¾extracted_textsä¸­ç²å–çš„ä½ç½®ä¿¡æ¯",
                    "importance": "high/medium/low",
                    "confidence": 0.95,
                    "char_count": 50,
                    "keywords": ["é—œéµè©1", "é—œéµè©2"]
                }}
            ],
            "table_data": [
                {{
                    "table_id": "table_001", 
                    "description": "è¡¨æ ¼æè¿°ï¼ˆå¾tables_detectedæå–ï¼‰",
                    "extracted_content": "è¡¨æ ¼çš„å…·é«”æ–‡å­—å…§å®¹",
                    "position": "è¡¨æ ¼åœ¨æ–‡æª”ä¸­çš„ä½ç½®",
                    "structured_data": {{
                        "åˆ—å1": "å€¼1",
                        "åˆ—å2": "å€¼2"
                    }},
                    "confidence": 0.9,
                    "importance": "high/medium/low"
                }}
            ],
            "chart_data": [
                {{
                    "chart_id": "chart_001",
                    "description": "åœ–è¡¨æè¿°",
                    "chart_type": "bar/line/pie/flow/other",
                    "extracted_text": "åœ–è¡¨ä¸­çš„æ–‡å­—å…§å®¹",
                    "position": "åœ–è¡¨ä½ç½®",
                    "insights": ["åœ–è¡¨æ´å¯Ÿ1", "åœ–è¡¨æ´å¯Ÿ2"],
                    "confidence": 0.85
                }}
            ],
            "layout_structure": {{
                "layout_type": "å¾layout_analysisæå–çš„ä½ˆå±€é¡å‹",
                "elements_hierarchy": [
                    {{
                        "element_type": "header/section/table/chart",
                        "position": "ä½ç½®",
                        "content_summary": "å…§å®¹æ¦‚è¦",
                        "importance": "high/medium/low"
                    }}
                ],
                "reading_order": ["å…ƒç´ 1", "å…ƒç´ 2", "å…ƒç´ 3"]
            }},
            "insights_with_source": [
                {{
                    "insight": "å¾content_analysis.data_insightsæå–çš„çœŸå¯¦æ´å¯Ÿ",
                    "source_evidence": "æ”¯æŒé€™å€‹æ´å¯Ÿçš„å…·é«”æ–‡æœ¬è­‰æ“š",
                    "confidence": 0.88,
                    "insight_type": "trend/comparison/conclusion/recommendation"
                }}
            ],
            "rag_optimized_chunks": [
                "åŸºæ–¼extracted_textsé‡æ–°çµ„ç¹”çš„èªç¾©æ–‡æœ¬å¡Š1ï¼šæ¨™é¡Œ+ç›¸é—œå…§å®¹",
                "èªç¾©æ–‡æœ¬å¡Š2ï¼šç‰¹å®šä¸»é¡Œçš„å®Œæ•´æ®µè½",
                "èªç¾©æ–‡æœ¬å¡Š3ï¼šè¡¨æ ¼è³‡æ–™+è§£é‡‹æ–‡å­—",
                "èªç¾©æ–‡æœ¬å¡Š4ï¼šåœ–è¡¨è³‡è¨Š+åˆ†æè¦é»"
            ],
            "metadata": {{
                "analysis_quality": "high/medium/low",
                "total_text_blocks": "å¾extracted_textsçµ±è¨ˆçš„å¯¦éš›æ•¸é‡",
                "total_tables": "å¯¦éš›æª¢æ¸¬åˆ°çš„è¡¨æ ¼æ•¸é‡",
                "total_charts": "å¯¦éš›æª¢æ¸¬åˆ°çš„åœ–è¡¨æ•¸é‡", 
                "confidence_score": 0.88,
                "processing_timestamp": "ç•¶å‰æ™‚é–“æˆ³",
                "tracing_enabled": true,
                "recommended_use_cases": ["åŸºæ–¼å¯¦éš›å…§å®¹æ¨è–¦çš„æ‡‰ç”¨å ´æ™¯"]
            }}
        }}

        **é‡è¦**ï¼š
        1. text_chunks_with_tracingå¿…é ˆåŒ…å«å¾extracted_textsæ•¸çµ„ä¸­æå–çš„**çœŸå¯¦æ–‡æœ¬å…§å®¹**
        2. æ¯å€‹è³‡æ–™å¡Šéƒ½è¦æœ‰æº¯æºè³‡è¨Šï¼ˆä½ç½®ã€é¡å‹ã€ç½®ä¿¡åº¦ï¼‰
        3. åˆ†åˆ¥è™•ç†æ–‡æœ¬ã€è¡¨æ ¼ã€åœ–è¡¨ä¸‰ç¨®ä¸åŒçš„è³‡æ–™æº
        4. rag_optimized_chunksè¦é‡æ–°çµ„ç¹”å…§å®¹ï¼ŒæŒ‰èªç¾©ç›¸é—œæ€§åˆ†çµ„
        5. ä¸è¦ä½¿ç”¨æ¨¡æ¿åŒ–å…§å®¹ï¼Œè¦åŸºæ–¼å¯¦éš›åˆ†æçµæœç”Ÿæˆ
        """
)

# æœ€çµ‚æ•´åˆæ™ºèƒ½é«”
upload_and_rag_index = SequentialAgent(
    name="document_rag_analyzer",
    sub_agents=[complete_analysis_workflow, rag_data_organizer, data_loader_agent, vectorization_agent ],
    description="å°ˆæ¥­æ–‡æª”åˆ†æç³»çµ±ï¼šåŸ·è¡Œå®Œæ•´åˆ†æä¸¦ç”ŸæˆRAGå‹å¥½çš„çµæ§‹åŒ–è³‡æ–™",
)

root_agent = SequentialAgent(
    name = 'qa_agent',
    sub_agents=[upload_and_rag_index, retrieval_agent, answer_generation_agent],
    description="QAæ™ºèƒ½é«”ï¼šä¸Šå‚³åœ–åƒä¸¦é€²è¡ŒRAGç´¢å¼•ï¼Œç„¶å¾Œé€²è¡Œæª¢ç´¢å’Œå›ç­”",
)